{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# packages used in this script         \n",
    "########################################\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Function: read_data()                                \n",
    "# Param:                                               \n",
    "#     - path (str): the path to the M&A news data file(s).\n",
    "# Return:\n",
    "#     - data (nparray): the input data read from the data path.\n",
    "###############################################################################################################################\n",
    "\n",
    "def read_data(path):\n",
    "    data = pd.read_csv(path, error_bad_lines=False)\n",
    "    data = data.to_numpy()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# Function: remove_inner_scheme()\n",
    "# Param:\n",
    "#     - data (nparray): the input data returned from the read_data() function\n",
    "# Return:\n",
    "#     - str_text (list): a list of cleaned news texts (which only contains text between <bodytext>)\n",
    "################################################################################################################################\n",
    "\n",
    "def remove_inner_scheme(data):\n",
    "    str_text = []\n",
    "\n",
    "    for row in data:\n",
    "        temp_text = row[3]\n",
    "\n",
    "        if type(temp_text) is not float:\n",
    "            soup = bs(temp_text, \"html.parser\")\n",
    "            cur_text = soup.findAll('bodytext')[0]\n",
    "\n",
    "            str_text.append(cur_text.text)\n",
    "\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# Function: train_LDA()\n",
    "# Param:\n",
    "#     - str_text (list): the list of cleaned news texts returned from the remove_inner_scheme() function\n",
    "# Return:\n",
    "#     - lda (model): the trained LDA model with pure M&A news.\n",
    "#     - cv (model): the trained CountVectorizer model with pure M&A news.\n",
    "################################################################################################################################\n",
    "\n",
    "def train_LDA(str_text):\n",
    "    # train test split (for now all the M&A news are training. No testing is needed.)\n",
    "    training_size = int(len(str_text) * 1.00)\n",
    "    test_size = len(str_text) - training_size\n",
    "    \n",
    "    training_data = random.sample(str_text, training_size)\n",
    "    test_data = []\n",
    "\n",
    "    for row in str_text:\n",
    "        if row not in training_data:\n",
    "            test_data.append(row)\n",
    "            \n",
    "    # main part for LDA\n",
    "    start_time = time.time()\n",
    "\n",
    "    cv = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')\n",
    "    df_train = cv.fit_transform(training_data[:])\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=10)\n",
    "    lda.fit(df_train)\n",
    "\n",
    "    print(\"running time: \" + str(time.time()-start_time))\n",
    "    return cv, lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# Function: print_topics()\n",
    "# Param:\n",
    "#     - lda (model): the trained LDA model returned from train_LDA() function.\n",
    "#     - cv (model): the trained CountVectorizer model returned from train_LDA() function.\n",
    "# Return:\n",
    "#     - None\n",
    "# Topics are printed in the stdout window\n",
    "################################################################################################################################\n",
    "\n",
    "def print_topics(lda, cv):\n",
    "    print(\"printing topics and words...\")\n",
    "    print(\"\")\n",
    "    for index, topic in enumerate(lda.components_):\n",
    "        print(f'Top 15 words for Topic #{index}')\n",
    "        print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code starting...\n",
      "Finish reading data...\n",
      "Finish cleaning inner schemes...\n",
      "running time: 23.26687979698181\n",
      "Finish training LDA model...\n",
      "printing topics and words...\n",
      "\n",
      "Top 15 words for Topic #0\n",
      "['real', 'based', 'ventures', 'rationale', 'estate', 'group', 'statuscompleted', 'venture', 'participantstarget', 'million', 'acquirer', 'management', 'investment', 'partners', 'capital']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #1\n",
      "['managers', 'gross', 'intends', 'use', 'senior', 'acted', '15', 'public', 'capital', 'llc', 'notes', 'proceeds', 'securities', 'million', 'offering']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #2\n",
      "['billion', '2018', 'merger', 'offering', 'capital', 'transaction', 'investment', 'said', 'intends', 'aramco', '2019', 'shares', 'ipo', 'saudi', 'update']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #3\n",
      "['llp', 'legal', 'announced', 'capital', 'advisor', 'group', '000', 'acting', 'price', 'share', 'stock', 'offering', 'million', 'common', 'shares']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #4\n",
      "['typeacquisitionsub', 'acquisition', 'llc', 'value', 'participantstarget', 'services', 'advisor', 'rationalethe', 'energy', 'oil', 'investment', 'statuscompleted', 'gas', 'million', 'transaction']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #5\n",
      "['statuscompleted', 'firm', 'equitysub', 'global', 'participantstarget', 'private', 'capital', 'financial', 'transaction', 'equity', 'rationale', 'services', 'group', 'based', 'million']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #6\n",
      "['round', 'rationale', 'ventures', 'value', 'participantstarget', 'statuscompleted', 'secured', 'financesub', 'typeventure', 'based', 'venture', 'partners', 'funding', 'capital', 'million']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #7\n",
      "['convertible', 'energy', 'investment', 'proteus', 'participantstarget', 'statuscompleted', 'medical', 'value', 'proceeds', 'health', 'group', 'ventures', 'investors', 'financing', 'million']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #8\n",
      "['oil', 'acquisition', 'offering', 'participantstarget', 'technology', 'common', 'placement', 'group', 'proceeds', 'energy', 'mm', 'price', 'share', 'million', 'shares']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #9\n",
      "['consideration', 'limited', 'holdings', 'value', 'typeacquisitionsub', 'stake', 'acquire', 'gas', 'oil', 'acquisition', 'energy', 'business', 'group', 'transaction', 'million']\n",
      "\n",
      "\n",
      "Models saved...\n",
      "Code finishing...\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "def main():\n",
    "    print(\"Code starting...\")\n",
    "    # define path to data file. (In local machine, this jupyter notebook is in the same level with folder \"data collection\")\n",
    "    path = \"data collection/M&A_news.csv\"\n",
    "    \n",
    "    # read data.\n",
    "    data = read_data(path)\n",
    "    print(\"Finish reading data...\")\n",
    "    \n",
    "    # clean inner scheme\n",
    "    str_text = remove_inner_scheme(data)\n",
    "    print(\"Finish cleaning inner schemes...\")\n",
    "    \n",
    "    # train LDA with pure M&A news\n",
    "    cv, lda = train_LDA(str_text)\n",
    "    print(\"Finish training LDA model...\")\n",
    "    \n",
    "    # print topics\n",
    "    print_topics(lda, cv)\n",
    "    \n",
    "    # save LDA model to \"lda.sav\" and CountVectorizer model to \"cv.sav\"\n",
    "    pickle.dump(lda, open('lda.sav', 'wb'))\n",
    "    pickle.dump(cv, open('cv.sav', 'wb'))\n",
    "    print(\"Models saved...\")\n",
    "    \n",
    "    print(\"Code finishing...\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
